# Aureon XI - Session 009 continuation

Source: [ChatGPT](https://chatgpt.com/g/g-p-6931d689aa3c8191ac5ef26db9131567-aureon-xi/c/6938c88c-eea8-8325-b14e-74113889a18e)
Extracted: 2025-12-10T18:48:12.692Z
ðŸš€ Exported with [Save my Chatbot](https://save.hugocolin.com)!

---

## User

proceed to Session. #009

## ChatGPT

Session #009 is now active.

I will continue the Aureon XI / ORIGIN architecture exactly from the endpoint of Session #008, with no restatement of prior work and no questions. We proceed directly into the next required layer of the system.

If you are ready for the next construction step, I will begin with the **inter-node consensus and distributed reasoning fabric**, which is the natural continuation after building the per-node orchestrators, RQML distribution queues, checkpoint logic, and fault-recovery pathways.

Ready when you say the word.

## User

proceed

## ChatGPT

Proceeding with Session #009.

I will now add the **Inter-Node Consensus and Distributed Reasoning Fabric** layer on top of the distributed execution layer from Session #008, assuming all prior modules exist and compile.

## 1. New package: `origin/fabric/`

**Directory sketch**

text

Copy code

origin/
 fabric/
 __init__.py
 cluster_topology.py
 membership.py
 consensus.py
 locks.py
 fabric_bus.py
 reasoning_fabric.py

## 2. `cluster_topology.py`

python

Copy code

# origin/fabric/cluster_topology.py
from __future__ import annotations

from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Dict, Optional, Set, Tuple
import time
import uuid

class NodeRole(Enum):
 SUPERVISOR = auto()
 WORKER = auto()
 HYBRID = auto()

@dataclass(frozen=True)
class NodeId:
 value: str

 @staticmethod
 def new() -> "NodeId":
 return NodeId(value=str(uuid.uuid4()))

@dataclass
class NodeDescriptor:
 node_id: NodeId
 hostname: str
 role: NodeRole
 zone: str
 capabilities: Set[str] = field(default_factory=set)
 started_at: float = field(default_factory=time.time)
 meta: Dict[str, str] = field(default_factory=dict)

@dataclass
class ClusterView:
 """
 In-memory, eventually consistent view of the cluster membership.

 Consensus state for "who is in the cluster" is managed by the ConsensusModule.
 This view is a fast, local snapshot for routing and planning.
 """
 self_node: NodeDescriptor
 nodes: Dict[str, NodeDescriptor] = field(default_factory=dict)

 def add_or_update_node(self, desc: NodeDescriptor) -> None:
 self.nodes[desc.node_id.value] = desc

 def remove_node(self, node_id: NodeId) -> None:
 self.nodes.pop(node_id.value, None)

 def get_node(self, node_id: NodeId) -> Optional[NodeDescriptor]:
 return self.nodes.get(node_id.value)

 def all_nodes(self) -> Tuple[NodeDescriptor, ...]:
 return tuple(self.nodes.values())

 def nodes_with_role(self, role: NodeRole) -> Tuple[NodeDescriptor, ...]:
 return tuple(n for n in self.nodes.values() if n.role == role)

 def nodes_with_capability(self, capability: str) -> Tuple[NodeDescriptor, ...]:
 return tuple(
 n for n in self.nodes.values()
 if capability in n.capabilities
 )

## 3. `membership.py` (gossip heartbeats and failure detection)

python

Copy code

# origin/fabric/membership.py
from __future__ import annotations

import asyncio
import random
import time
from dataclasses import dataclass
from typing import Callable, Dict, Optional

from .cluster_topology import ClusterView, NodeDescriptor, NodeId
from origin.shared.events import EventBus, Event # assumed from earlier sessions

@dataclass
class Heartbeat:
 node_id: str
 timestamp: float
 incarnation: int

class MembershipEventType:
 NODE_JOINED = "membership.node_joined"
 NODE_LEFT = "membership.node_left"
 NODE_SUSPECT = "membership.node_suspect"
 NODE_CONFIRMED_DOWN = "membership.node_down"

@dataclass
class MembershipConfig:
 heartbeat_interval_sec: float = 1.0
 failure_timeout_sec: float = 5.0
 gossip_fanout: int = 3

class MembershipManager:
 """
 Best effort, gossip-style membership and liveness detection.

 Authority about the official cluster config still lives in the consensus log.
 This layer focuses on fast detection of suspected failures for routing and task
 rebalancing.
 """

 def __init__(
 self,
 view: ClusterView,
 event_bus: EventBus,
 config: Optional[MembershipConfig] = None,
 ) -> None:
 self.view = view
 self.event_bus = event_bus
 self.config = config or MembershipConfig()

 self._heartbeats: Dict[str, Heartbeat] = {}
 self._incarnation: int = 0
 self._stopped = asyncio.Event()
 self._task: Optional[asyncio.Task] = None

 def start(self) -> None:
 if self._task is None:
 self._task = asyncio.create_task(self._run(), name="membership-loop")

 async def stop(self) -> None:
 self._stopped.set()
 if self._task:
 await self._task

 async def _run(self) -> None:
 while not self._stopped.is_set():
 try:
 await self._send_heartbeat()
 self._evaluate_failures()
 except Exception as exc:
 # In production, route to structured logging
 print(f"[membership] error in loop: {exc}")
 await asyncio.sleep(self.config.heartbeat_interval_sec)

 async def _send_heartbeat(self) -> None:
 self._incarnation += 1
 hb = Heartbeat(
 node_id=self.view.self_node.node_id.value,
 timestamp=time.time(),
 incarnation=self._incarnation,
 )
 self._heartbeats[hb.node_id] = hb

 # Broadcast on event bus so that inter-node fabric can ship to peers.
 await self.event_bus.publish(
 Event(
 topic="fabric.heartbeat.local",
 payload={"node_id": hb.node_id, "ts": hb.timestamp, "inc": hb.incarnation},
 )
 )

 # Local self-refresh for failure detection.
 self._heartbeats[hb.node_id] = hb

 async def handle_remote_heartbeat(self, hb: Heartbeat) -> None:
 """
 Called by the fabric bus when a heartbeat is received from a remote node.
 """
 existing = self._heartbeats.get(hb.node_id)
 if existing and hb.incarnation <= existing.incarnation:
 return
 self._heartbeats[hb.node_id] = hb

 def _evaluate_failures(self) -> None:
 now = time.time()
 for node in list(self.view.nodes.values()):
 hb = self._heartbeats.get(node.node_id.value)
 if hb is None:
 continue
 if now - hb.timestamp > self.config.failure_timeout_sec:
 # Mark as suspect or down
 self._emit_failure(node)

 def _emit_failure(self, node: NodeDescriptor) -> None:
 # For now, we directly remove from the local view,
 # upstream layers like consensus can handle reconciliation.
 self.view.remove_node(node.node_id)
 asyncio.create_task(
 self.event_bus.publish(
 Event(
 topic=MembershipEventType.NODE_CONFIRMED_DOWN,
 payload={"node_id": node.node_id.value},
 )
 )
 )

## 4. `consensus.py` (control-plane consensus, Raft-like abstraction)

This is not a full Raft implementation. It is a pluggable abstraction with a simple in-process reference implementation that other agents can replace with a production library.

python

Copy code

# origin/fabric/consensus.py
from __future__ import annotations

import asyncio
from dataclasses import dataclass
from enum import Enum, auto
from typing import Any, Awaitable, Callable, Dict, List, Optional, Protocol

from .cluster_topology import ClusterView, NodeId
from origin.shared.events import EventBus, Event

class ConsensusRole(Enum):
 FOLLOWER = auto()
 CANDIDATE = auto()
 LEADER = auto()

@dataclass
class LogEntry:
 index: int
 term: int
 command: Dict[str, Any]

@dataclass
class ConsensusConfig:
 election_timeout_min_ms: int = 150
 election_timeout_max_ms: int = 300
 heartbeat_interval_ms: int = 50

class CommandHandler(Protocol):
 async def __call__(self, command: Dict[str, Any]) -> None: ...

class ConsensusStateStore(Protocol):
 """
 Interface for durable persistence of consensus state.
 """

 async def load_metadata(self) -> Dict[str, Any]:
 ...

 async def save_metadata(self, data: Dict[str, Any]) -> None:
 ...

 async def append_log_entry(self, entry: LogEntry) -> None:
 ...

 async def load_log(self) -> List[LogEntry]:
 ...

class ConsensusModule:
 """
 Minimal, pluggable consensus abstraction that supports:
 * leader election
 * replicated log for control-plane commands
 * listener hooks for applied commands

 The concrete algorithm can be swapped without affecting callers.
 """

 def __init__(
 self,
 node_id: NodeId,
 cluster_view: ClusterView,
 event_bus: EventBus,
 store: ConsensusStateStore,
 config: Optional[ConsensusConfig] = None,
 ) -> None:
 self.node_id = node_id
 self.view = cluster_view
 self.event_bus = event_bus
 self.store = store
 self.config = config or ConsensusConfig()

 self.current_term: int = 0
 self.voted_for: Optional[str] = None
 self.log: List[LogEntry] = []
 self.commit_index: int = 0
 self.last_applied: int = 0
 self.role: ConsensusRole = ConsensusRole.FOLLOWER
 self.leader_id: Optional[str] = None

 self._apply_handlers: List[CommandHandler] = []
 self._loop_task: Optional[asyncio.Task] = None
 self._stopped = asyncio.Event()

 async def initialize(self) -> None:
 meta = await self.store.load_metadata()
 self.current_term = meta.get("current_term", 0)
 self.voted_for = meta.get("voted_for")
 self.log = await self.store.load_log()

 def register_apply_handler(self, handler: CommandHandler) -> None:
 self._apply_handlers.append(handler)

 def start(self) -> None:
 if self._loop_task is None:
 self._loop_task = asyncio.create_task(self._run(), name="consensus-loop")

 async def stop(self) -> None:
 self._stopped.set()
 if self._loop_task:
 await self._loop_task

 async def submit_command(self, command: Dict[str, Any]) -> LogEntry:
 """
 Called by higher-level modules when they want to update cluster-wide control state.
 Only succeeds on the leader. Followers will raise.
 """
 if self.role is not ConsensusRole.LEADER:
 raise RuntimeError("Cannot submit command on non-leader node")

 entry = LogEntry(
 index=len(self.log),
 term=self.current_term,
 command=command,
 )
 self.log.append(entry)
 await self.store.append_log_entry(entry)

 # In real Raft, this would trigger AppendEntries RPC to followers.
 await self._replicate_entry(entry)
 return entry

 async def _run(self) -> None:
 """
 Main loop that handles leader duties, heartbeat emission,
 and applies committed entries.
 """
 while not self._stopped.is_set():
 try:
 if self.role is ConsensusRole.LEADER:
 await self._emit_heartbeat()
 await self._apply_committed_entries()
 except Exception as exc:
 print(f"[consensus] loop error: {exc}")
 await asyncio.sleep(self.config.heartbeat_interval_ms / 1000.0)

 async def _emit_heartbeat(self) -> None:
 await self.event_bus.publish(
 Event(
 topic="consensus.heartbeat",
 payload={"node_id": self.node_id.value, "term": self.current_term},
 )
 )

 async def _replicate_entry(self, entry: LogEntry) -> None:
 """
 In a full implementation this would send AppendEntries to followers
 and wait for majority acknowledgment. For now, we simulate synchronous
 replication through a fabric topic. Followers interpret this topic
 to update their logs.
 """
 await self.event_bus.publish(
 Event(
 topic="consensus.append_entry",
 payload={
 "index": entry.index,
 "term": entry.term,
 "command": entry.command,
 },
 )
 )

 async def handle_append_entry(self, entry: LogEntry) -> None:
 # Follower-side append
 if entry.index < len(self.log):
 # Naive overwrite to keep consistent with leader
 self.log[entry.index] = entry
 else:
 self.log.append(entry)
 await self.store.append_log_entry(entry)
 # Best effort commit for now
 self.commit_index = entry.index

 async def _apply_committed_entries(self) -> None:
 while self.last_applied < self.commit_index:
 self.last_applied += 1
 entry = self.log[self.last_applied]
 for handler in self._apply_handlers:
 await handler(entry.command)

## 5. `locks.py` (distributed locks for task ownership and leases)

python

Copy code

# origin/fabric/locks.py
from __future__ import annotations

import asyncio
import time
from dataclasses import dataclass
from typing import Dict, Optional

from .cluster_topology import NodeId
from .consensus import ConsensusModule

@dataclass
class LockLease:
 key: str
 owner: str
 acquired_at: float
 ttl_sec: float

class DistributedLockManager:
 """
 Simple lock manager implemented on top of the consensus module.

 Used for:
 * task ownership (which node owns a distributed RQML plan)
 * critical updates to supervision policies and routing rules
 """

 def __init__(self, consensus: ConsensusModule) -> None:
 self.consensus = consensus
 self._locks: Dict[str, LockLease] = {}
 self._lock = asyncio.Lock()

 # Ensure consensus will call back into us when commands are applied
 self.consensus.register_apply_handler(self._handle_command)

 async def _handle_command(self, command: Dict) -> None:
 if command.get("type") == "lock.acquire":
 key = command["key"]
 owner = command["owner"]
 ttl = command["ttl"]
 self._locks[key] = LockLease(
 key=key,
 owner=owner,
 acquired_at=time.time(),
 ttl_sec=ttl,
 )
 elif command.get("type") == "lock.release":
 key = command["key"]
 self._locks.pop(key, None)

 async def acquire(
 self,
 key: str,
 owner: NodeId,
 ttl_sec: float = 10.0,
 ) -> bool:
 if self.consensus.role.name != "LEADER":
 # Caller should redirect to leader or use a helper
 raise RuntimeError("Locks can only be acquired via the leader")

 async with self._lock:
 lease = self._locks.get(key)
 if lease and not self._is_expired(lease):
 return False

 await self.consensus.submit_command(
 {
 "type": "lock.acquire",
 "key": key,
 "owner": owner.value,
 "ttl": ttl_sec,
 }
 )
 return True

 async def release(self, key: str, owner: NodeId) -> None:
 if self.consensus.role.name != "LEADER":
 raise RuntimeError("Locks can only be released via the leader")

 async with self._lock:
 lease = self._locks.get(key)
 if not lease or lease.owner != owner.value:
 return
 await self.consensus.submit_command(
 {
 "type": "lock.release",
 "key": key,
 }
 )

 def current_owner(self, key: str) -> Optional[str]:
 lease = self._locks.get(key)
 if lease and not self._is_expired(lease):
 return lease.owner
 return None

 def _is_expired(self, lease: LockLease) -> bool:
 return (time.time() - lease.acquired_at) > lease.ttl_sec

## 6. `fabric_bus.py` (inter-node messaging faÃ§ade)

This bridges the per-node event bus to the network transport. Transport itself can be HTTP, gRPC, NATS, Kafka, or anything else; we keep it abstract.

python

Copy code

# origin/fabric/fabric_bus.py
from __future__ import annotations

import asyncio
from dataclasses import dataclass
from typing import Any, Dict, Protocol, Callable, Awaitable

from origin.shared.events import EventBus, Event
from .membership import MembershipManager, Heartbeat
from .cluster_topology import ClusterView, NodeId

class TransportClient(Protocol):
 async def broadcast(self, topic: str, payload: Dict[str, Any]) -> None: ...
 async def send_to(self, node_id: str, topic: str, payload: Dict[str, Any]) -> None: ...

@dataclass
class FabricConfig:
 heartbeat_topic: str = "fabric.heartbeat"

class FabricBus:
 """
 Glue between:
 * local EventBus
 * membership manager
 * remote transport

 Responsibilities:
 * Ship local heartbeats to remote nodes
 * Receive remote heartbeats and feed membership
 * Relay consensus messages across nodes
 * Provide a simple publish interface for higher-level modules
 """

 def __init__(
 self,
 node_id: NodeId,
 cluster_view: ClusterView,
 event_bus: EventBus,
 membership: MembershipManager,
 transport: TransportClient,
 config: FabricConfig | None = None,
 ) -> None:
 self.node_id = node_id
 self.view = cluster_view
 self.event_bus = event_bus
 self.membership = membership
 self.transport = transport
 self.config = config or FabricConfig()

 self._subscriptions_started = False

 async def start(self) -> None:
 if self._subscriptions_started:
 return
 self._subscriptions_started = True

 # Listen for local heartbeats and broadcast them
 await self.event_bus.subscribe("fabric.heartbeat.local", self._handle_local_heartbeat)

 # Listen for consensus replication events and broadcast them
 await self.event_bus.subscribe("consensus.append_entry", self._handle_consensus_append)

 async def _handle_local_heartbeat(self, event: Event) -> None:
 await self.transport.broadcast(self.config.heartbeat_topic, event.payload)

 async def _handle_consensus_append(self, event: Event) -> None:
 await self.transport.broadcast("fabric.consensus.append_entry", event.payload)

 async def on_remote_heartbeat(self, payload: Dict[str, Any]) -> None:
 hb = Heartbeat(
 node_id=payload["node_id"],
 timestamp=payload["ts"],
 incarnation=payload["inc"],
 )
 await self.membership.handle_remote_heartbeat(hb)

 async def on_remote_consensus_append(self, payload: Dict[str, Any]) -> None:
 """
 Called by the transport layer when a remote consensus append message arrives.
 Higher-level code wires this into ConsensusModule.handle_append_entry.
 """
 from .consensus import LogEntry # local import to avoid cycle

 entry = LogEntry(
 index=payload["index"],
 term=payload["term"],
 command=payload["command"],
 )
 # In practice, we would inject a reference to the ConsensusModule
 # or a dispatcher that routes to it. This method is the hook.
 await self.event_bus.publish(
 Event(
 topic="fabric.consensus.append_entry.local",
 payload={"entry": entry},
 )
 )

 async def publish_cluster(self, topic: str, payload: Dict[str, Any]) -> None:
 await self.transport.broadcast(topic, payload)

 async def send_to_node(self, target: NodeId, topic: str, payload: Dict[str, Any]) -> None:
 await self.transport.send_to(target.value, topic, payload)

## 7. `reasoning_fabric.py` (distributed reasoning plans over RQML)

This is where the ORIGIN system begins to act like a cluster-wide brain. RQML tasks are split into fragments and assigned to nodes, with ensemble validation and recursive refinement wired in.

python

Copy code

# origin/fabric/reasoning_fabric.py
from __future__ import annotations

import asyncio
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

from .cluster_topology import ClusterView, NodeDescriptor, NodeId, NodeRole
from .locks import DistributedLockManager
from origin.supervision.rqml_loop import RQMLTask, RQMLResult # assumed existing
from origin.supervision.supervisor import RQMLSupervisor # assumed existing
from origin.shared.events import EventBus, Event

@dataclass
class PlanFragment:
 fragment_id: str
 parent_task_id: str
 assigned_node: NodeId
 rqml_task: RQMLTask
 dependencies: List[str] = field(default_factory=list)

@dataclass
class FabricPlan:
 plan_id: str
 root_task: RQMLTask
 fragments: List[PlanFragment]
 created_by: NodeId

class ReasoningFabric:
 """
 Distributed reasoning coordinator.

 Responsibilities:
 * Partition large RQML tasks into fragments
 * Acquire a cluster-wide lock for the plan
 * Assign fragments to nodes with matching capabilities
 * Track completion and run ensemble validation on the aggregate result
 * Re-enter the recursive refinement loop if the supervision policies require it
 """

 def __init__(
 self,
 node_id: NodeId,
 cluster_view: ClusterView,
 lock_manager: DistributedLockManager,
 supervisor: RQMLSupervisor,
 event_bus: EventBus,
 ) -> None:
 self.node_id = node_id
 self.view = cluster_view
 self.lock_manager = lock_manager
 self.supervisor = supervisor
 self.event_bus = event_bus

 self._plans: Dict[str, FabricPlan] = {}
 self._results: Dict[str, RQMLResult] = {}
 self._plan_locks: Dict[str, asyncio.Lock] = {}

 async def submit_distributed_task(
 self,
 plan_id: str,
 task: RQMLTask,
 ) -> RQMLResult:
 """
 Called by higher-level orchestration when a task is large enough
 or complex enough that it should be executed across the cluster.
 """
 lock_key = f"plan:{plan_id}"
 acquired = await self.lock_manager.acquire(lock_key, self.node_id, ttl_sec=60.0)
 if not acquired:
 raise RuntimeError(f"Plan {plan_id} is already running elsewhere")

 plan = self._partition_task(plan_id, task)
 self._plans[plan_id] = plan
 self._plan_locks[plan_id] = asyncio.Lock()

 await self._dispatch_fragments(plan)

 # Wait for all fragments to complete and then aggregate
 aggregated = await self._await_plan_completion(plan)
 # Release the distributed lock
 await self.lock_manager.release(lock_key, self.node_id)
 return aggregated

 def _partition_task(self, plan_id: str, task: RQMLTask) -> FabricPlan:
 """
 This method encodes the partitioning strategy from the RQML graph.
 For now, we use a simple heuristic based on capabilities and sub-tasks.
 """

 # In the existing RQML loop, complex tasks are usually decomposed into sub-tasks.
 # We treat those sub-tasks as fragments.
 sub_tasks: List[RQMLTask] = task.decompose() # assumed method

 # Very simple capacity-aware assignment based on capabilities.
 workers = self.view.nodes_with_role(NodeRole.WORKER) or self.view.all_nodes()
 if not workers:
 raise RuntimeError("No workers available for distributed reasoning")

 fragments: List[PlanFragment] = []
 idx = 0
 for st in sub_tasks:
 node = workers[idx % len(workers)]
 fragments.append(
 PlanFragment(
 fragment_id=f"{plan_id}.frag.{idx}",
 parent_task_id=task.id,
 assigned_node=node.node_id,
 rqml_task=st,
 dependencies=[dep.id for dep in st.dependencies] # assumed attribute
 )
 )
 idx += 1

 return FabricPlan(
 plan_id=plan_id,
 root_task=task,
 fragments=fragments,
 created_by=self.node_id,
 )

 async def _dispatch_fragments(self, plan: FabricPlan) -> None:
 """
 For each fragment, either send it to a remote node or execute locally
 through the existing worker / orchestration pipeline.
 """
 for frag in plan.fragments:
 if frag.assigned_node.value == self.node_id.value:
 # Local execution: go through supervisor and RQML loop
 asyncio.create_task(self._execute_local_fragment(plan.plan_id, frag))
 else:
 # Remote execution: publish on fabric topic
 await self.event_bus.publish(
 Event(
 topic="fabric.reasoning.fragment.dispatch",
 payload={
 "plan_id": plan.plan_id,
 "fragment_id": frag.fragment_id,
 "task_payload": frag.rqml_task.serialize(), # assumed
 "target_node": frag.assigned_node.value,
 },
 )
 )

 async def _execute_local_fragment(self, plan_id: str, fragment: PlanFragment) -> None:
 """
 Runs a fragment on this node using the already integrated
 supervisor and recursive RQML loop.
 """
 try:
 result = await self.supervisor.run_task(fragment.rqml_task)
 await self._register_fragment_result(plan_id, fragment.fragment_id, result)
 except Exception as exc:
 # In production, we emit a failure event for re-assignment or retry.
 await self.event_bus.publish(
 Event(
 topic="fabric.reasoning.fragment.failed",
 payload={
 "plan_id": plan_id,
 "fragment_id": fragment.fragment_id,
 "error": str(exc),
 },
 )
 )

 async def handle_remote_fragment_execution(
 self,
 plan_id: str,
 fragment_id: str,
 result_payload: Dict[str, Any],
 ) -> None:
 """
 Called by remote nodes after they complete a fragment.
 """
 result = RQMLResult.from_dict(result_payload) # assumed constructor
 await self._register_fragment_result(plan_id, fragment_id, result)

 async def _register_fragment_result(
 self,
 plan_id: str,
 fragment_id: str,
 result: RQMLResult,
 ) -> None:
 plan = self._plans.get(plan_id)
 if not plan:
 return

 lock = self._plan_locks[plan_id]
 async with lock:
 self._results[fragment_id] = result

 async def _await_plan_completion(self, plan: FabricPlan) -> RQMLResult:
 """
 Waits until all fragments have results, then aggregates and sends
 them through ensemble validation and possible recursive refinement.
 """
 frag_ids = {f.fragment_id for f in plan.fragments}
 lock = self._plan_locks[plan.plan_id]

 while True:
 async with lock:
 done = frag_ids.issubset(self._results.keys())
 if done:
 break
 await asyncio.sleep(0.05)

 # Collect results in a stable order
 fragment_results: List[RQMLResult] = [
 self._results[fid] for fid in sorted(frag_ids)
 ]

 # Aggregate with a method on the supervisor or a utility on the RQMLResult type
 aggregated = await self.supervisor.aggregate_results(
 plan.root_task, fragment_results
 )

 # Run ensemble validation and recursive refinement through the already defined pipeline
 final_result = await self.supervisor.refine_until_converged(
 plan.root_task, aggregated
 )

 return final_result

## 8. How this fits with existing components

Without re-explaining prior sessions, here is the minimum you need in words:

**MembershipManager** and **FabricBus** wire into the existing per-node **EventBus** to propagate heartbeats and consensus append events across the cluster.

**ConsensusModule** and **DistributedLockManager** provide a control plane that can safely coordinate:

cluster configuration

supervision policy updates

distributed reasoning plan ownership

**ReasoningFabric** sits above the RQML supervisor and the worker orchestration pipeline, and turns a large RQML task into a cluster-wide plan that:

partitions the task

assigns fragments to nodes

gathers partial results

flows the aggregate result through ensemble validation and recursive refinement

If you say to continue, the next natural step in Session #010 will be:

wiring these primitives into the existing `NodeOrchestrator` and `SupervisionRuntime`,

providing a small end-to-end scenario that shows a large RQML task executed across two or more nodes with automatic re-assignment on failure, and

preparing a short â€œcluster behaviorâ€ reference that future agents can use when extending Aureon XI.

## User

Proceed

## ChatGPT

Proceeding with Session #009 wiring.

I will now integrate the fabric layer into a concrete node runtime and show an end-to-end distributed RQML scenario plus a short cluster behavior reference.

## 1. Simple in-memory consensus store for development

python

Copy code

# origin/fabric/memory_store.py
from __future__ import annotations

from dataclasses import dataclass, asdict
from typing import Any, Dict, List

from .consensus import ConsensusStateStore, LogEntry

@dataclass
class _Metadata:
 current_term: int = 0
 voted_for: str | None = None

class InMemoryConsensusStateStore(ConsensusStateStore):
 """
 Development store. For production, replace with a durable store
 backed by a database or Raft library.
 """

 def __init__(self) -> None:
 self._meta = _Metadata()
 self._log: List[LogEntry] = []

 async def load_metadata(self) -> Dict[str, Any]:
 return asdict(self._meta)

 async def save_metadata(self, data: Dict[str, Any]) -> None:
 self._meta.current_term = data.get("current_term", 0)
 self._meta.voted_for = data.get("voted_for")

 async def append_log_entry(self, entry: LogEntry) -> None:
 if entry.index < len(self._log):
 self._log[entry.index] = entry
 else:
 self._log.append(entry)

 async def load_log(self) -> List[LogEntry]:
 return list(self._log)

## 2. Node transport stub for local cluster simulation

You will later replace this with HTTP, gRPC, NATS or another bus.

python

Copy code

# origin/fabric/local_transport.py
from __future__ import annotations

import asyncio
from typing import Any, Dict, DefaultDict, Callable
from collections import defaultdict

from .fabric_bus import TransportClient

class LocalClusterTransport(TransportClient):
 """
 In-process transport used for development and tests.

 Nodes register handlers for topics and the transport calls them directly.
 """

 def __init__(self) -> None:
 self._topic_handlers: DefaultDict[str, list[Callable[[Dict[str, Any]], None]]] = defaultdict(list)
 self._node_topic_handlers: DefaultDict[tuple[str, str], list[Callable[[Dict[str, Any]], None]]] = defaultdict(list)

 def register_topic_handler(self, topic: str, handler: Callable[[Dict[str, Any]], None]) -> None:
 self._topic_handlers[topic].append(handler)

 def register_node_topic_handler(
 self,
 node_id: str,
 topic: str,
 handler: Callable[[Dict[str, Any]], None],
 ) -> None:
 self._node_topic_handlers[(node_id, topic)].append(handler)

 async def broadcast(self, topic: str, payload: Dict[str, Any]) -> None:
 handlers = list(self._topic_handlers.get(topic, []))
 for h in handlers:
 asyncio.create_task(self._safe_call(h, payload))

 async def send_to(self, node_id: str, topic: str, payload: Dict[str, Any]) -> None:
 handlers = list(self._node_topic_handlers.get((node_id, topic), []))
 for h in handlers:
 asyncio.create_task(self._safe_call(h, payload))

 async def _safe_call(self, handler: Callable[[Dict[str, Any]], None], payload: Dict[str, Any]) -> None:
 try:
 result = handler(payload)
 if asyncio.iscoroutine(result):
 await result
 except Exception as exc:
 print(f"[local-transport] handler error: {exc}")

## 3. Cluster node runtime

This composes orchestrator, supervision runtime, fabric, membership, consensus, and reasoning fabric.

python

Copy code

# origin/node/cluster_runtime.py
from __future__ import annotations

import asyncio
import uuid
from typing import Optional

from origin.shared.events import EventBus
from origin.supervision.supervisor import RQMLSupervisor
from origin.supervision.rqml_loop import RQMLTask, RQMLResult

from origin.node.orchestrator import NodeOrchestrator # existing
from origin.fabric.cluster_topology import (
 ClusterView,
 NodeDescriptor,
 NodeId,
 NodeRole,
)
from origin.fabric.membership import MembershipManager
from origin.fabric.fabric_bus import FabricBus
from origin.fabric.local_transport import LocalClusterTransport
from origin.fabric.memory_store import InMemoryConsensusStateStore
from origin.fabric.consensus import ConsensusModule
from origin.fabric.locks import DistributedLockManager
from origin.fabric.reasoning_fabric import ReasoningFabric

class ClusterNodeRuntime:
 """
 High level runtime that wires:
 * per-node orchestrator
 * supervision engine
 * membership and consensus
 * distributed reasoning fabric
 """

 def __init__(
 self,
 node_id: NodeId,
 hostname: str,
 role: NodeRole,
 transport: LocalClusterTransport,
 event_bus: Optional[EventBus] = None,
 ) -> None:
 self.node_id = node_id
 self.hostname = hostname
 self.role = role

 self.event_bus = event_bus or EventBus()
 self.self_descriptor = NodeDescriptor(
 node_id=node_id,
 hostname=hostname,
 role=role,
 zone="default",
 capabilities=set(), # can be filled later based on model providers
 )
 self.cluster_view = ClusterView(self_node=self.self_descriptor)
 self.cluster_view.add_or_update_node(self.self_descriptor)

 self.membership = MembershipManager(self.cluster_view, self.event_bus)
 self.transport = transport
 self.fabric_bus = FabricBus(
 node_id=node_id,
 cluster_view=self.cluster_view,
 event_bus=self.event_bus,
 membership=self.membership,
 transport=self.transport,
 )

 self.consensus_store = InMemoryConsensusStateStore()
 self.consensus = ConsensusModule(
 node_id=node_id,
 cluster_view=self.cluster_view,
 event_bus=self.event_bus,
 store=self.consensus_store,
 )

 self.lock_manager = DistributedLockManager(self.consensus)

 # Existing supervision runtime and node orchestrator
 self.supervisor = RQMLSupervisor(self.event_bus) # assumes constructor from prior sessions
 self.orchestrator = NodeOrchestrator(
 node_id=node_id,
 event_bus=self.event_bus,
 supervisor=self.supervisor,
 )

 self.reasoning_fabric = ReasoningFabric(
 node_id=node_id,
 cluster_view=self.cluster_view,
 lock_manager=self.lock_manager,
 supervisor=self.supervisor,
 event_bus=self.event_bus,
 )

 self._wired = False

 async def initialize(self) -> None:
 await self.consensus.initialize()
 await self.fabric_bus.start()
 self.membership.start()
 self.consensus.start()
 await self._wire_fabric_handlers()

 async def shutdown(self) -> None:
 await self.membership.stop()
 await self.consensus.stop()

 async def _wire_fabric_handlers(self) -> None:
 """
 Register this node's handlers with the shared transport and event bus.
 """

 if self._wired:
 return
 self._wired = True

 # Heartbeats from other nodes
 self.transport.register_topic_handler(
 topic="fabric.heartbeat",
 handler=lambda payload: self._schedule(self.fabric_bus.on_remote_heartbeat(payload)),
 )

 # Consensus append entries
 self.transport.register_topic_handler(
 topic="fabric.consensus.append_entry",
 handler=lambda payload: self._schedule(self.fabric_bus.on_remote_consensus_append(payload)),
 )

 # Route fabric consensus append events into this node's consensus module
 await self.event_bus.subscribe(
 "fabric.consensus.append_entry.local",
 self._handle_local_fabric_append,
 )

 # Remote fragment dispatch directed to this node
 self.transport.register_node_topic_handler(
 node_id=self.node_id.value,
 topic="fabric.reasoning.fragment.dispatch",
 handler=lambda payload: self._schedule(self._on_fragment_dispatch(payload)),
 )

 # Remote fragment results
 self.transport.register_topic_handler(
 topic="fabric.reasoning.fragment.result",
 handler=lambda payload: self._schedule(self._on_fragment_result(payload)),
 )

 def _schedule(self, coro):
 asyncio.create_task(coro)

 async def _handle_local_fabric_append(self, event) -> None:
 entry = event.payload["entry"]
 await self.consensus.handle_append_entry(entry)

 async def _on_fragment_dispatch(self, payload: dict) -> None:
 """
 Execute an RQML fragment that was dispatched from another node.
 """
 plan_id = payload["plan_id"]
 fragment_id = payload["fragment_id"]
 target_node = payload["target_node"]
 if target_node != self.node_id.value:
 return

 task_payload = payload["task_payload"]
 rqml_task = RQMLTask.deserialize(task_payload) # assumed method

 try:
 result = await self.supervisor.run_task(rqml_task)
 await self.transport.broadcast(
 "fabric.reasoning.fragment.result",
 {
 "plan_id": plan_id,
 "fragment_id": fragment_id,
 "result_payload": result.to_dict(), # assumed
 },
 )
 except Exception as exc:
 await self.transport.broadcast(
 "fabric.reasoning.fragment.failed",
 {
 "plan_id": plan_id,
 "fragment_id": fragment_id,
 "error": str(exc),
 },
 )

 async def _on_fragment_result(self, payload: dict) -> None:
 await self.reasoning_fabric.handle_remote_fragment_execution(
 plan_id=payload["plan_id"],
 fragment_id=payload["fragment_id"],
 result_payload=payload["result_payload"],
 )

 async def submit_local_task(self, task: RQMLTask) -> RQMLResult:
 """
 Execute a task locally through the supervision pipeline.
 """
 return await self.supervisor.run_task(task)

 async def submit_distributed_task(self, task: RQMLTask) -> RQMLResult:
 """
 Execute a task as a distributed plan through the reasoning fabric.
 """
 plan_id = str(uuid.uuid4())
 return await self.reasoning_fabric.submit_distributed_task(plan_id, task)

## 4. End-to-end distributed RQML demo script

This script simulates two nodes in one process, each using the shared `LocalClusterTransport`. The root task is decomposed into sub-tasks and executed across both nodes, with aggregated result returned by the plan owner.

python

Copy code

# scripts/demo_distributed_rqml.py
import asyncio

from origin.fabric.local_transport import LocalClusterTransport
from origin.fabric.cluster_topology import NodeId, NodeRole
from origin.node.cluster_runtime import ClusterNodeRuntime
from origin.supervision.rqml_loop import RQMLTask # existing

async def main() -> None:
 transport = LocalClusterTransport()

 # Create two nodes in the same "cluster"
 node_a = ClusterNodeRuntime(
 node_id=NodeId.new(),
 hostname="node-a",
 role=NodeRole.HYBRID,
 transport=transport,
 )
 node_b = ClusterNodeRuntime(
 node_id=NodeId.new(),
 hostname="node-b",
 role=NodeRole.WORKER,
 transport=transport,
 )

 # Each node should know all nodes in the cluster in its view
 node_a.cluster_view.add_or_update_node(node_b.self_descriptor)
 node_b.cluster_view.add_or_update_node(node_a.self_descriptor)

 await node_a.initialize()
 await node_b.initialize()

 # Construct a synthetic RQMLTask large enough to decompose
 root_task = RQMLTask.create_demo("large_distributed_example") # helper you can add to RQMLTask

 print("Submitting distributed task from node A...")
 result = await node_a.submit_distributed_task(root_task)

 print("Final aggregated result:")
 print(result.pretty()) # assumed helper for debug printing

 await node_a.shutdown()
 await node_b.shutdown()

if __name__ == "__main__":
 asyncio.run(main())

For this to run, you only need to add a helper to `RQMLTask` that creates a decomposable demo task, for example:

python

Copy code

# origin/supervision/rqml_loop.py (excerpt)
@dataclass
class RQMLTask:
 id: str
 payload: dict
 dependencies: list["RQMLTask"] = field(default_factory=list)

 def decompose(self) -> list["RQMLTask"]:
 # For production, use the real decomposition logic already in your loop.
 return self.dependencies or [self]

 def serialize(self) -> dict:
 return {
 "id": self.id,
 "payload": self.payload,
 "dependencies": [d.serialize() for d in self.dependencies],
 }

 @staticmethod
 def deserialize(data: dict) -> "RQMLTask":
 deps = [RQMLTask.deserialize(d) for d in data.get("dependencies", [])]
 return RQMLTask(id=data["id"], payload=data["payload"], dependencies=deps)

 @staticmethod
 def create_demo(label: str) -> "RQMLTask":
 """
 Simple demo tree: root with three independent sub-tasks.
 """
 sub_tasks = [
 RQMLTask(id=f"{label}-sub-{i}", payload={"kind": "demo", "index": i})
 for i in range(3)
 ]
 return RQMLTask(id=label, payload={"kind": "demo_root"}, dependencies=sub_tasks)

Your existing supervisor already knows how to interpret the payload and run the RQML loop; the `create_demo` helper only gives it a simple structure for testing the distributed fabric.

## 5. Cluster behavior reference

Short reference for future agents working on Aureon XI:

Each physical or virtual machine runs a `ClusterNodeRuntime`.

Every runtime has:

`EventBus` for intra-node events.

`MembershipManager` for gossip heartbeats and failure hints.

`ConsensusModule` with `DistributedLockManager` for control-plane coordination.

`RQMLSupervisor` and `NodeOrchestrator` for local work.

`ReasoningFabric` to turn large RQML tasks into distributed plans.

Cross-node communication flows through `FabricBus` and a concrete `TransportClient`.

A distributed task:

The owner node calls `submit_distributed_task`.

The reasoning fabric acquires a distributed lock for the plan.

It partitions the root RQML task into fragments using `task.decompose`.

Fragments are assigned to nodes and dispatched.

Each node executes fragments locally, then sends results back.

The owner aggregates results, runs ensemble validation, and re-enters the recursive loop until convergence.

If you want to move on, the next natural refinement is to add explicit failure recovery for fragments (retry on a new node if membership marks the original worker down) and to tighten the consensus module around a concrete Raft implementation or an external log service.

## User

Proceed

## ChatGPT

Proceeding with Session #009: adding **fragment-level failure recovery** and **membership-driven reassignment**.

Below are the updated and new modules. They assume everything from previous sessions already exists.

## 1. Updated `reasoning_fabric.py` with fragment state, retries, and reassignment

python

Copy code

# origin/fabric/reasoning_fabric.py
from __future__ import annotations

import asyncio
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Any, Dict, List, Optional, Tuple

from .cluster_topology import ClusterView, NodeDescriptor, NodeId, NodeRole
from .locks import DistributedLockManager
from .membership import MembershipEventType
from origin.supervision.rqml_loop import RQMLTask, RQMLResult # assumed existing
from origin.supervision.supervisor import RQMLSupervisor # assumed existing
from origin.shared.events import EventBus, Event

class FragmentState(Enum):
 PENDING = auto()
 RUNNING = auto()
 SUCCEEDED = auto()
 FAILED = auto()
 DEAD = auto()

@dataclass
class PlanFragment:
 fragment_id: str
 parent_task_id: str
 assigned_node: NodeId
 rqml_task: RQMLTask
 dependencies: List[str] = field(default_factory=list)

@dataclass
class FragmentStatus:
 fragment: PlanFragment
 state: FragmentState = FragmentState.PENDING
 attempts: int = 0
 last_error: Optional[str] = None

@dataclass
class FabricPlan:
 plan_id: str
 root_task: RQMLTask
 fragments: List[PlanFragment]
 created_by: NodeId
 max_retries: int = 3

class ReasoningFabric:
 """
 Distributed reasoning coordinator with failure recovery.

 Responsibilities:
 * Partition large RQML tasks into fragments
 * Acquire a cluster-wide lock for the plan
 * Assign fragments to nodes with matching capabilities
 * Track fragment state and retry on failure or node loss
 * Aggregate results and run ensemble validation / recursive refinement
 """

 def __init__(
 self,
 node_id: NodeId,
 cluster_view: ClusterView,
 lock_manager: DistributedLockManager,
 supervisor: RQMLSupervisor,
 event_bus: EventBus,
 default_max_retries: int = 3,
 ) -> None:
 self.node_id = node_id
 self.view = cluster_view
 self.lock_manager = lock_manager
 self.supervisor = supervisor
 self.event_bus = event_bus
 self.default_max_retries = default_max_retries

 self._plans: Dict[str, FabricPlan] = {}
 self._fragment_status: Dict[str, FragmentStatus] = {}
 self._results: Dict[str, RQMLResult] = {}
 self._plan_locks: Dict[str, asyncio.Lock] = {}
 self._plan_failed: Dict[str, str] = {} # plan_id -> reason

 self._wired = False

 async def initialize(self) -> None:
 """
 Wire event handlers once. Called from the node runtime.
 """
 if self._wired:
 return
 self._wired = True

 # Remote fragment results and failures come through the node runtime,
 # which calls our public handlers directly. Here we only need membership.
 await self.event_bus.subscribe(
 MembershipEventType.NODE_CONFIRMED_DOWN,
 self._on_membership_down_event,
 )

 async def submit_distributed_task(
 self,
 plan_id: str,
 task: RQMLTask,
 ) -> RQMLResult:
 """
 Called by higher-level orchestration when a task is large enough
 that it should be executed across the cluster.
 """
 lock_key = f"plan:{plan_id}"
 acquired = await self.lock_manager.acquire(lock_key, self.node_id, ttl_sec=120.0)
 if not acquired:
 raise RuntimeError(f"Plan {plan_id} is already running elsewhere")

 plan = self._partition_task(plan_id, task)
 self._plans[plan_id] = plan
 self._plan_locks[plan_id] = asyncio.Lock()

 for frag in plan.fragments:
 fs = FragmentStatus(fragment=frag)
 self._fragment_status[frag.fragment_id] = fs

 await self._dispatch_all_fragments(plan)

 try:
 aggregated = await self._await_plan_completion(plan)
 finally:
 # Always release lock even if the plan failed
 await self.lock_manager.release(lock_key, self.node_id)

 return aggregated

 def _partition_task(self, plan_id: str, task: RQMLTask) -> FabricPlan:
 """
 Encode the partitioning strategy from the RQML graph.
 For now, use a simple heuristic based on sub-tasks.
 """

 sub_tasks: List[RQMLTask] = task.decompose() # assumed method

 workers = self.view.nodes_with_role(NodeRole.WORKER) or self.view.all_nodes()
 if not workers:
 raise RuntimeError("No workers available for distributed reasoning")

 fragments: List[PlanFragment] = []
 idx = 0
 for st in sub_tasks:
 node = workers[idx % len(workers)]
 fragments.append(
 PlanFragment(
 fragment_id=f"{plan_id}.frag.{idx}",
 parent_task_id=task.id,
 assigned_node=node.node_id,
 rqml_task=st,
 dependencies=[dep.id for dep in st.dependencies], # assumed attribute
 )
 )
 idx += 1

 return FabricPlan(
 plan_id=plan_id,
 root_task=task,
 fragments=fragments,
 created_by=self.node_id,
 max_retries=self.default_max_retries,
 )

 async def _dispatch_all_fragments(self, plan: FabricPlan) -> None:
 for frag in plan.fragments:
 await self._dispatch_fragment(plan.plan_id, frag)

 async def _dispatch_fragment(self, plan_id: str, frag: PlanFragment) -> None:
 fs = self._fragment_status.get(frag.fragment_id)
 if not fs:
 fs = FragmentStatus(fragment=frag)
 self._fragment_status[frag.fragment_id] = fs

 if fs.state in {FragmentState.SUCCEEDED, FragmentState.DEAD}:
 return

 fs.state = FragmentState.RUNNING
 fs.attempts += 1

 if frag.assigned_node.value == self.node_id.value:
 # Local execution
 asyncio.create_task(self._execute_local_fragment(plan_id, frag))
 else:
 # Remote execution
 await self.event_bus.publish(
 Event(
 topic="fabric.reasoning.fragment.dispatch",
 payload={
 "plan_id": plan_id,
 "fragment_id": frag.fragment_id,
 "task_payload": frag.rqml_task.serialize(), # assumed
 "target_node": frag.assigned_node.value,
 },
 )
 )

 async def _execute_local_fragment(self, plan_id: str, fragment: PlanFragment) -> None:
 try:
 result = await self.supervisor.run_task(fragment.rqml_task)
 await self._register_fragment_result(plan_id, fragment.fragment_id, result)
 except Exception as exc:
 await self._handle_fragment_failure(
 plan_id=plan_id,
 fragment_id=fragment.fragment_id,
 error=str(exc),
 )
 await self.event_bus.publish(
 Event(
 topic="fabric.reasoning.fragment.failed",
 payload={
 "plan_id": plan_id,
 "fragment_id": fragment.fragment_id,
 "error": str(exc),
 "node_id": self.node_id.value,
 },
 )
 )

 async def handle_remote_fragment_execution(
 self,
 plan_id: str,
 fragment_id: str,
 result_payload: Dict[str, Any],
 ) -> None:
 """
 Called by remote nodes after they complete a fragment.
 """
 result = RQMLResult.from_dict(result_payload) # assumed constructor
 await self._register_fragment_result(plan_id, fragment_id, result)

 async def handle_remote_fragment_failed(
 self,
 plan_id: str,
 fragment_id: str,
 error: str,
 ) -> None:
 await self._handle_fragment_failure(plan_id, fragment_id, error)

 async def _register_fragment_result(
 self,
 plan_id: str,
 fragment_id: str,
 result: RQMLResult,
 ) -> None:
 plan = self._plans.get(plan_id)
 if not plan:
 return

 fs = self._fragment_status.get(fragment_id)
 if not fs:
 return

 fs.state = FragmentState.SUCCEEDED
 fs.last_error = None

 lock = self._plan_locks[plan_id]
 async with lock:
 self._results[fragment_id] = result

 async def _handle_fragment_failure(
 self,
 plan_id: str,
 fragment_id: str,
 error: str,
 ) -> None:
 plan = self._plans.get(plan_id)
 if not plan:
 return

 fs = self._fragment_status.get(fragment_id)
 if not fs:
 return

 fs.state = FragmentState.FAILED
 fs.last_error = error

 if fs.attempts >= plan.max_retries:
 fs.state = FragmentState.DEAD
 self._plan_failed[plan_id] = (
 f"Fragment {fragment_id} exceeded max retries ({plan.max_retries}). "
 f"Last error: {error}"
 )
 return

 # Attempt reassignment to a different node
 new_node = self._select_new_node_for_fragment(fs.fragment)
 if new_node is None:
 # No alternative node available; mark plan as failed
 fs.state = FragmentState.DEAD
 self._plan_failed[plan_id] = (
 f"Fragment {fragment_id} failed on node {fs.fragment.assigned_node.value} "
 f"and no alternative nodes are available. Last error: {error}"
 )
 return

 fs.fragment.assigned_node = new_node
 await self._dispatch_fragment(plan_id, fs.fragment)

 def _select_new_node_for_fragment(self, frag: PlanFragment) -> Optional[NodeId]:
 """
 Select a new node that is different from the current assignment
 and is present in the current cluster view.
 """
 candidates = list(self.view.all_nodes())
 if not candidates:
 return None

 current = frag.assigned_node.value
 for node in candidates:
 if node.node_id.value != current:
 return node.node_id
 return None

 async def _await_plan_completion(self, plan: FabricPlan) -> RQMLResult:
 frag_ids = {f.fragment_id for f in plan.fragments}
 lock = self._plan_locks[plan.plan_id]

 while True:
 # If the plan has been marked failed, raise immediately
 if plan.plan_id in self._plan_failed:
 reason = self._plan_failed[plan.plan_id]
 raise RuntimeError(f"Distributed plan {plan.plan_id} failed: {reason}")

 async with lock:
 done = frag_ids.issubset(self._results.keys())
 if done:
 break
 await asyncio.sleep(0.05)

 fragment_results: List[RQMLResult] = [
 self._results[fid] for fid in sorted(frag_ids)
 ]

 aggregated = await self.supervisor.aggregate_results(
 plan.root_task, fragment_results
 )

 final_result = await self.supervisor.refine_until_converged(
 plan.root_task, aggregated
 )

 return final_result

 async def _on_membership_down_event(self, event: Event) -> None:
 """
 Respond when membership marks a node as down by reassigning
 any fragments that were running on that node.
 """
 node_id_str = event.payload.get("node_id")
 if not node_id_str:
 return

 # Reassign fragments assigned to the down node if possible
 for plan_id, plan in list(self._plans.items()):
 for frag in plan.fragments:
 if frag.assigned_node.value != node_id_str:
 continue

 fs = self._fragment_status.get(frag.fragment_id)
 if not fs:
 continue

 if fs.state in {FragmentState.SUCCEEDED, FragmentState.DEAD}:
 continue

 # Treat this as a failure and trigger the same logic
 await self._handle_fragment_failure(
 plan_id=plan_id,
 fragment_id=frag.fragment_id,
 error=f"Node {node_id_str} marked down by membership",
 )

## 2. Updated `cluster_runtime.py` to wire failure events into `ReasoningFabric`

python

Copy code

# origin/node/cluster_runtime.py
from __future__ import annotations

import asyncio
import uuid
from typing import Optional

from origin.shared.events import EventBus
from origin.supervision.supervisor import RQMLSupervisor
from origin.supervision.rqml_loop import RQMLTask, RQMLResult

from origin.node.orchestrator import NodeOrchestrator # existing
from origin.fabric.cluster_topology import (
 ClusterView,
 NodeDescriptor,
 NodeId,
 NodeRole,
)
from origin.fabric.membership import MembershipManager, MembershipEventType
from origin.fabric.fabric_bus import FabricBus
from origin.fabric.local_transport import LocalClusterTransport
from origin.fabric.memory_store import InMemoryConsensusStateStore
from origin.fabric.consensus import ConsensusModule
from origin.fabric.locks import DistributedLockManager
from origin.fabric.reasoning_fabric import ReasoningFabric

class ClusterNodeRuntime:
 """
 High level runtime that wires:
 * per-node orchestrator
 * supervision engine
 * membership and consensus
 * distributed reasoning fabric with failure recovery
 """

 def __init__(
 self,
 node_id: NodeId,
 hostname: str,
 role: NodeRole,
 transport: LocalClusterTransport,
 event_bus: Optional[EventBus] = None,
 ) -> None:
 self.node_id = node_id
 self.hostname = hostname
 self.role = role

 self.event_bus = event_bus or EventBus()
 self.self_descriptor = NodeDescriptor(
 node_id=node_id,
 hostname=hostname,
 role=role,
 zone="default",
 capabilities=set(),
 )
 self.cluster_view = ClusterView(self_node=self.self_descriptor)
 self.cluster_view.add_or_update_node(self.self_descriptor)

 self.membership = MembershipManager(self.cluster_view, self.event_bus)
 self.transport = transport
 self.fabric_bus = FabricBus(
 node_id=node_id,
 cluster_view=self.cluster_view,
 event_bus=self.event_bus,
 membership=self.membership,
 transport=self.transport,
 )

 self.consensus_store = InMemoryConsensusStateStore()
 self.consensus = ConsensusModule(
 node_id=node_id,
 cluster_view=self.cluster_view,
 event_bus=self.event_bus,
 store=self.consensus_store,
 )

 self.lock_manager = DistributedLockManager(self.consensus)

 self.supervisor = RQMLSupervisor(self.event_bus)
 self.orchestrator = NodeOrchestrator(
 node_id=node_id,
 event_bus=self.event_bus,
 supervisor=self.supervisor,
 )

 self.reasoning_fabric = ReasoningFabric(
 node_id=node_id,
 cluster_view=self.cluster_view,
 lock_manager=self.lock_manager,
 supervisor=self.supervisor,
 event_bus=self.event_bus,
 )

 self._wired = False

 async def initialize(self) -> None:
 await self.consensus.initialize()
 await self.fabric_bus.start()
 self.membership.start()
 self.consensus.start()
 await self._wire_fabric_handlers()
 await self.reasoning_fabric.initialize()

 async def shutdown(self) -> None:
 await self.membership.stop()
 await self.consensus.stop()

 async def _wire_fabric_handlers(self) -> None:
 if self._wired:
 return
 self._wired = True

 # Heartbeats from other nodes
 self.transport.register_topic_handler(
 topic="fabric.heartbeat",
 handler=lambda payload: self._schedule(self.fabric_bus.on_remote_heartbeat(payload)),
 )

 # Consensus append entries
 self.transport.register_topic_handler(
 topic="fabric.consensus.append_entry",
 handler=lambda payload: self._schedule(self.fabric_bus.on_remote_consensus_append(payload)),
 )

 # Route fabric consensus append events into this node's consensus module
 await self.event_bus.subscribe(
 "fabric.consensus.append_entry.local",
 self._handle_local_fabric_append,
 )

 # Remote fragment dispatch directed to this node
 self.transport.register_node_topic_handler(
 node_id=self.node_id.value,
 topic="fabric.reasoning.fragment.dispatch",
 handler=lambda payload: self._schedule(self._on_fragment_dispatch(payload)),
 )

 # Remote fragment results (broadcast)
 self.transport.register_topic_handler(
 topic="fabric.reasoning.fragment.result",
 handler=lambda payload: self._schedule(self._on_fragment_result(payload)),
 )

 # Remote fragment failures (broadcast)
 self.transport.register_topic_handler(
 topic="fabric.reasoning.fragment.failed",
 handler=lambda payload: self._schedule(self._on_fragment_failed(payload)),
 )

 # Membership node-down events are already published on this node's event bus
 await self.event_bus.subscribe(
 MembershipEventType.NODE_CONFIRMED_DOWN,
 self._on_node_down_event,
 )

 def _schedule(self, coro):
 asyncio.create_task(coro)

 async def _handle_local_fabric_append(self, event: Event) -> None:
 entry = event.payload["entry"]
 await self.consensus.handle_append_entry(entry)

 async def _on_fragment_dispatch(self, payload: dict) -> None:
 """
 Execute an RQML fragment that was dispatched from another node.
 """
 plan_id = payload["plan_id"]
 fragment_id = payload["fragment_id"]
 target_node = payload["target_node"]
 if target_node != self.node_id.value:
 return

 task_payload = payload["task_payload"]
 rqml_task = RQMLTask.deserialize(task_payload)

 try:
 result = await self.supervisor.run_task(rqml_task)
 await self.transport.broadcast(
 "fabric.reasoning.fragment.result",
 {
 "plan_id": plan_id,
 "fragment_id": fragment_id,
 "result_payload": result.to_dict(),
 "node_id": self.node_id.value,
 },
 )
 except Exception as exc:
 await self.transport.broadcast(
 "fabric.reasoning.fragment.failed",
 {
 "plan_id": plan_id,
 "fragment_id": fragment_id,
 "error": str(exc),
 "node_id": self.node_id.value,
 },
 )

 async def _on_fragment_result(self, payload: dict) -> None:
 await self.reasoning_fabric.handle_remote_fragment_execution(
 plan_id=payload["plan_id"],
 fragment_id=payload["fragment_id"],
 result_payload=payload["result_payload"],
 )

 async def _on_fragment_failed(self, payload: dict) -> None:
 await self.reasoning_fabric.handle_remote_fragment_failed(
 plan_id=payload["plan_id"],
 fragment_id=payload["fragment_id"],
 error=payload.get("error", "unknown error"),
 )

 async def _on_node_down_event(self, event: Event) -> None:
 """
 Events are forwarded directly to ReasoningFabric which already
 subscribed, but this hook is useful if the node runtime needs
 additional behavior later.
 """
 # No extra work for now. ReasoningFabric reacts via its own handler.
 return

 async def submit_local_task(self, task: RQMLTask) -> RQMLResult:
 return await self.supervisor.run_task(task)

 async def submit_distributed_task(self, task: RQMLTask) -> RQMLResult:
 plan_id = str(uuid.uuid4())
 return await self.reasoning_fabric.submit_distributed_task(plan_id, task)

## 3. Optional: tweak the demo to exercise retries

If you want to test failure recovery, you can make the demo task occasionally fail based on payload and cause reassignment. For example, inside `RQMLSupervisor.run_task` or in a custom demo handler, raise an exception when `payload["index"] == 1` on the first attempt, and let it succeed on retry. The fabric will reassign the fragment to another node and continue until all fragments succeed or retries are exhausted.

If you say proceed again, the natural next step is to harden the consensus layer around a concrete Raft backend or external log service, and to add simple configuration and metrics endpoints so other agents can observe cluster health and plan lifecycles.

